# -*- coding: utf-8 -*-
"""作業三_財政四107205093許恩嘉.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ewUsrYRjD-iDmCGv8ssywFc9iJD_ZFUa
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#one hot encoding
from tensorflow.keras.utils import to_categorical
# standard process to build a deep learning model
from tensorflow.keras.models import Sequential
# Dense is Fully Connected Layer
from tensorflow.keras.layers import Dense
# Gradient Descent
from tensorflow.keras.optimizers import SGD

"""### 1. 讀入 MNSIT 數據集"""

from tensorflow.keras.datasets import mnist

#y_train
mnist.load_data()[0][1]

(x_train, y_train), (x_test, y_test) = mnist.load_data()

"""#### 你也可以讀入 Fasion 版的 MNIST"""

# from tensorflow.keras.datasets import fashion_mnist
# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

"""### 2. 欣賞數據集內容 (非執行必要)"""

len(x_train)

len(x_test)

n = 9487


plt.imshow(x_train[n], cmap='Greys')

"""### 3. 資料整理

先看個範例, 因為 `numpy` 「廣播」的特性, 我們對 `array` 中所有數字要同除以一個數可瞬間完成!
"""

x_train.shape

# /255 is Normalization 
x_train = x_train.reshape(60000, 784)/255
x_test = x_test.reshape(10000, 784)/255

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

"""### 4. step 1: 打造神經網路"""

model = Sequential()

model.add(Dense(100, input_dim=784, activation='relu'))

model.add(Dense(100, activation='relu'))

model.add(Dense(10, activation='softmax'))

"""#### 組裝我們的神經網路"""

#用categorical_crossentropy當我們的損失函數
#多元分類適合用，如果是二元可以用binary categroical_crossentropy
#categorical_crossentropy的原理是：
# 1.最後一層的hidden_layer產出後經過softmax activation後加機率壓縮到[0-1]
# 2.公式為: -(sigma(yi*log(yi_hat)))
# 3.依照公式可以知道：當softmax的向量 = y_hat的向量時，categorical_crossentropy=0
# 4.會在公式最外層加一個負號原因是經過softmax後數值為0,1之間，取log後0.6會大於0.4，和我們要的效果相反，因此取負號
# 5.第四點可見下圖
#優化器以adam取代GSD，不需要一開始就去調整lr，看看訓練結果再決定
model.compile(loss='categorical_crossentropy', optimizer='adam', 
              metrics=['accuracy'])

import warnings
warnings.filterwarnings('ignore')
range = np.arange(0,10,0.01)
range_ori = np.log(range)
range_mod = [np.log(r) if r>=1 else (-np.log(r)) for r in range]
plt.plot(range, range_ori)
plt.plot(range, range_mod)
plt.axhline(0,ls ='--',lw=2,color='green')
plt.show()

model.summary()

"""### 5. step 2: 訓練"""

model.fit(x_train, y_train, batch_size=100, epochs=20)

"""#訓練結果差不多是99%，不需調整lr

### 6. step 3: 預測 (testing)
"""

predict = np.argmax(model.predict(x_test),axis=-1)

n = 9999

print('神經網路預測是:', predict[n])
plt.imshow(x_test[n].reshape(28,28), cmap='Greys');

"""### 7. 儲存我們的 model"""

model.save("my_DNN.h5")

import tensorflow as tf

tf.test.gpu_device_name()

